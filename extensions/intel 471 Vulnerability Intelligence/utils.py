import itertools
import re
import sys
import structlog
import iso3166
import requests
from dev_kit.eiq_edk.schemas._utils import ExtractType
from dev_kit.eiq_edk.schemas._validation import valid_domain, valid_ipv4_cidr, valid_ipv6_cidr, valid_url
from validators import sha256, sha1, sha512, md5, ipv4, ipv6, email

HEADERS = {}

REPORT_ENDPOINT = "reports/{}"
HTTP_REQUEST_TIMEOUT = 120
PAGE_SIZE = 100
# Description needs to be under 18MB, because there are too big images
DESCRIPTION_SIZE_LIMIT = 18874368
# Description has multiple images that are over 10MB and package fails.
# We remove every image above 1MB.
IMAGE_SIZE_LIMIT = 10485760

country_list = (
    "czech republic",
    "russia",
    "united states",
    "great britain",
    "united kingdom",
    "latvija",
)


log = structlog.get_logger(__name__)

def batch(iterable, size):
    """Break given iterable into chunks of given size. Generator."""
    it = iter(iterable)
    while True:
        chunk = list(itertools.islice(it, size))
        if not chunk:
            break
        yield chunk


def fetch_with_paging(
    self,
    api_url,
    count_col_name,
    root_node,
    created_col_name,
    auth,
    query_params,
    verify_ssl,
    page_size=PAGE_SIZE,
):
    offset = 0
    last_report_timestamp = query_params["from"]
    while True:
        query_params.update({"count": page_size, "offset": offset})
        response = fetch_results(
            url=api_url, auth=auth, verify_ssl=verify_ssl, params=query_params
        )
        if response:
            item_count = response.get(count_col_name, 0)
            items = response.get(root_node, [])

            for item in items:
                if root_node == "posts":
                    item["searched_actor"] = query_params.get("actor")

                # allow both [activity.first|activity.last] and [date|created|etc]
                # to be passed as created_col_name argument
                parts = created_col_name.split(".")
                if len(parts) > 1:
                    parent = item.get(parts[0])
                    field = parts[1]
                else:
                    parent = item
                    field = parts[0]
                last_report_timestamp = parent.get(field, last_report_timestamp)

                yield item

            if item_count <= offset + page_size:
                break
        else:
            self.send_error({
                "code": "404",
                "description": "Not Found error, we handle that with custom exception on first",
                "message": f"API returned error for {api_url}"})
            raise Intel471Exception(
                'Provided parameters result with "404: not found" error'
            )

        if offset + page_size > 1000:
            # offset is out the range [0-1000]
            # reset offset and move time filters
            offset = 0
            query_params.update({"from": last_report_timestamp, "offset": offset})
        else:
            offset += page_size


def fetch_results(self, url, auth, verify_ssl, ext_type="Provider", **params):
    try:
        response = requests.get(
            url=url,
            auth=auth,
            headers=HEADERS,
            timeout=HTTP_REQUEST_TIMEOUT,
            verify=verify_ssl,
            **params,
        )
    except requests.Timeout:

        self.send_error({
            "code": "ERR-0000",
            "description": f"{ext_type}  failed, service timeout",
            "message": f"{response.text}"})
        raise
    if not response.ok:
        if response.status_code == 404:
            self.send_error({
                "code": "ERR-0000",
                "description": f"{ext_type}  failed, service unavailable",
                "message": f"{response.text}"})
            return {}
        handle_errors(response, ext_type)
        response.raise_for_status()
    try:
        data = response.json()
        # some blobs come in too big - fields bellow can be larger than 20mb,
        # which is blob size limit.. so instead of rejecting the whole blob, we'll
        # just drop some fields, if they're too big.
        # Some images also come with large base64 encoding and package fails,
        # so we remove that image to ingest the report
        the_text = ""
        complete_fields_size = (
            data.get("researcherComments", "")
            + data.get("rawText", "")
            + data.get("rawTextTranslated", "")
        )

        if sys.getsizeof(complete_fields_size) > DESCRIPTION_SIZE_LIMIT:
            for field in ["researcherComments", "rawText", "rawTextTranslated"]:
                remove_images_from_description(data, field)
                if (
                    sys.getsizeof(data.get(field, "") + the_text)
                    < DESCRIPTION_SIZE_LIMIT
                ):
                    the_text += data.get(field, "")
                else:
                    data.pop(field)
    except ValueError:
        self.send_error({
            "code": "ERR-0000",
            "description": f"{ext_type} failed",
            "message": f"unexpected data type encountered"})
        raise
    return data


def remove_images_from_description(data, field):
    matches = re.findall('<img.*?src="(.*?)"[^>]+>', data.get(field, ""))
    for match in matches:
        if sys.getsizeof(match) > IMAGE_SIZE_LIMIT:
            data[field].replace(match, "")


def handle_errors(self, response, ext_type):
    # if file is not found, don't stop the feed
    if response.status_code in (401, 403):

        self.send_error({
            "code": "401 / 403",
            "description": f"{ext_type} failed, authentication error",
            "message": f"{response.text} {response.status_code}."
        })
        pass
    elif response.status_code > 500:
        self.send_error({
            "code": "500",
            "description": f"{ext_type} failed, service unavailable",
            "message": f"{response.text}."
        })
        pass
    else:
        self.send_error({
            "code": "ERR-0000",
            "description": f"{ext_type} failed, malformed request",
            "message": f"{response.text}."
        })
        pass


def fetch_with_cursor(
    self,
    api_url,
    root_node,
    auth,
    query_params,
    verify_ssl,
):
    response = fetch_results(
        self, url=api_url, auth=auth, verify_ssl=verify_ssl, params=query_params
    )
    if response:
        while response.get("cursorNext"):
            if not response.get(root_node, []):
                break
            items = response.get(root_node, [])
            for item in items:
                yield item
            query_params["cursor"] = response.get("cursorNext")
            response = fetch_results(
                url=api_url, auth=auth, verify_ssl=verify_ssl, params=query_params
            )
    else:
        self.send_error({
          "code": "404",
          "description": "3rd party connector error",
          "message": f"An error occured during contacting 3rd party  {api_url}. Aborting."
        })
        pass

    if response.get("indicatorTotalCount") == 0:

        self.send_warning({
            "code": "WAR-0001",
            "message": "No results",
            "description": "Provider finished with no results."
        })
        pass






def create_extract(
    kind: ExtractType,
    value: str,
    maliciousness: str = None,
    classification: str = None,
    diff: str = None,
    link_type: str = "observed",
) -> dict:
    if value is None or any(
        [
            kind is None,
            kind == ExtractType.URI and not valid_url(value),
            kind == ExtractType.DOMAIN and not valid_domain(value),
            kind == ExtractType.EMAIL and not email(value),
            kind == ExtractType.IPV4 and not ipv4(value),
            kind == ExtractType.IPV4_CIDR and not valid_ipv4_cidr(value),
            kind == ExtractType.IPV6 and not ipv6(value),
            kind == ExtractType.IPV6_CIDR and not valid_ipv6_cidr(value),
            kind == ExtractType.HASH_MD5 and not md5(value),
            kind == ExtractType.HASH_SHA1 and not sha1(value),
            kind == ExtractType.HASH_SHA256 and not sha256(value),
            kind == ExtractType.HASH_SHA512 and not sha512(value),
            kind == ExtractType.COUNTRY
            and value.lower() not in country_list
            and iso3166.countries.get(value, None) is None,
            kind == ExtractType.COUNTRY_CODE
            and iso3166.countries.get(value, None) is None,
        ]
    ):
        log.error(f"Validation didn't pass for {kind} with this value {value}")
        return {}

    extract = {
        "kind": kind.value,
        "value": check_value(str(value), kind),
        "link_type": link_type,
    }
    # if maliciousness is given, then classification is "bad"
    if maliciousness and maliciousness in ["low", "medium", "high"]:
        extract["classification"] = "bad"
        extract["confidence"] = maliciousness
    if classification and classification in ["unknown", "good"]:
        extract["classification"] = classification
        extract.pop("confidence", None)
    if diff:
        extract["diff"] = diff
    return extract


def check_value(value, kind):
    if kind in (
        ExtractType.DOMAIN,
        ExtractType.EMAIL,
        ExtractType.CWE,
        ExtractType.CVE,
        ExtractType.HASH_MD5,
        ExtractType.HASH_SHA1,
        ExtractType.HASH_SHA256,
        ExtractType.HASH_SHA512,
    ):
        value = value.lower()
        if kind in (ExtractType.CVE, ExtractType.CWE):
            value = value.replace("cve-", "").replace("cwe-", "")
    elif kind in (ExtractType.INDUSTRY,):
        value = value.title()
    elif kind in (ExtractType.COUNTRY_CODE,):
        value = value.upper()
    return value


class Intel471Exception(Exception):
    def __init__(self, arg):
        self.strerror = arg
        self.args = {arg}
